# -*- coding: utf-8 -*-
"""Smol_VLM_FT_V2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JL9MSMGU2pRUvR9ZUBh46aatuSTOMHbg

<a href="https://colab.research.google.com/github/merveenoyan/smollm/blob/main/vision/finetuning/Smol_VLM_FT.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

!pip install -q accelerate datasets peft bitsandbytes tensorboard

!pip install -q flash-attn==2.7.3 #--no-build-isolation

pip install -U datasets

import matplotlib.pyplot as plt
import os
from huggingface_hub import login
import torch
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import torch
from tqdm import tqdm
from PIL import Image
import re

os.environ["HF_TOKEN"] = ""
login(token=os.environ["HF_TOKEN"], add_to_git_credential=True)

USE_LORA = False
USE_QLORA = True
SMOL = True

model_id = "HuggingFaceTB/SmolVLM-Base" if SMOL else "HuggingFaceM4/Idefics3-8B-Llama3"

processor = AutoProcessor.from_pretrained(
    model_id
)

if USE_QLORA or USE_LORA:
    lora_config = LoraConfig(
        r=8,
        lora_alpha=8,
        lora_dropout=0.1,
        target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],
        use_dora=False if USE_QLORA else True,
        init_lora_weights="gaussian"
    )
    lora_config.inference_mode = False
    if USE_QLORA:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

    model = Idefics3ForConditionalGeneration.from_pretrained(
        model_id,
        quantization_config=bnb_config if USE_QLORA else None,
        _attn_implementation="flash_attention_2",
        device_map="auto"
    )
    model.add_adapter(lora_config)
    model.enable_adapters()
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    print(model.get_nb_trainable_parameters())
else:
    model = Idefics3ForConditionalGeneration.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        _attn_implementation="flash_attention_2",
    ).to(DEVICE)


    for param in model.model.vision_model.parameters():
        param.requires_grad = False

"""The model as is is holding 2.7 GB of GPU RAM ðŸ’—

##Â Loading the dataset and Preprocessing
"""

repo_id = "Harsh9699/Stanford_car_75_25_split"
ds = load_dataset(repo_id)

# Get class names from the dataset features
label_feature = ds["train"].features["label"]
class_names = label_feature.names

# Add class names to the dataset
def add_class_name(example):
    example["class_name"] = class_names[example["label"]]
    return example

ds = ds.map(add_class_name)

# Use only training split
train_ds = ds["train"]

# COLLATE FUNCTION
image_token_id = processor.tokenizer.additional_special_tokens_ids[
    processor.tokenizer.additional_special_tokens.index("<image>")]

def collate_fn(examples):
    texts = []
    images = []
    for example in examples:
        image = example["image"]
        if image.mode != 'RGB':
            image = image.convert('RGB')

        # Use car class name from the new column
        car_class = example["class_name"]

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "What is the make, model, and year of this car?"},
                    {"type": "image"}
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": car_class}
                ]
            }
        ]
        text = processor.apply_chat_template(messages, add_generation_prompt=False)
        texts.append(text.strip())
        images.append([image])

    batch = processor(text=texts, images=images, return_tensors="pt", padding=True)
    labels = batch["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100
    labels[labels == image_token_id] = -100
    batch["labels"] = labels

    return batch

"""## Training"""

from transformers import TrainingArguments, Trainer

model_name = model_id.split("/")[-1]

training_args = TrainingArguments(
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=50,
    learning_rate=1e-4,
    weight_decay=0.01,
    logging_steps=25,
    save_strategy="steps",
    save_steps=250,
    save_total_limit=1,
    optim="paged_adamw_8bit",
    bf16=True,
    output_dir=f"./{model_name}-vqav2",
    hub_model_id=f"{model_name}-vqav2",
    report_to="tensorboard",
    remove_unused_columns=False,
    gradient_checkpointing=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    train_dataset=train_ds,
)

trainer.train()

# Extract training logs
train_logs = trainer.state.log_history

# Prepare data for plotting
steps = []
losses = []
for log in train_logs:
    if 'loss' in log:
        steps.append(log['step'])
        losses.append(log['loss'])

# Create and save plot
plt.figure(figsize=(10, 6))
plt.plot(steps, losses, marker='o', linestyle='-', color='b')
plt.title('Training Loss vs. Steps')
plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.grid(True)
plt.tight_layout()

# Save plot to file
plt.savefig('training_loss.png')
plt.show()

trainer.push_to_hub("Harsh9699/fine_tune_smolVLM")

def evaluate_model(model, processor, dataset, num_samples=50):
    """
    Evaluate fine-tuned model on random samples and display results

    """
    # Set model to evaluation mode
    model.eval()

    # Select random samples and convert indices to Python integers
    indices = np.random.choice(len(dataset), num_samples, replace=False)
    samples = [dataset[int(i)] for i in indices]  # Convert numpy int to Python int

    # Storage for results
    predictions = []
    actuals = []
    images = []

    # Generate predictions
    with torch.no_grad():
        for sample in tqdm(samples, desc="Evaluating samples"):
            # Get image and convert to RGB
            image = sample["image"]
            if image.mode != 'RGB':
                image = image.convert('RGB')

            # Get actual class name
            actual_class = sample["class_name"]
            actuals.append(actual_class)
            images.append(image.copy())

            # Create prompt
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "What is the make, model, and year of this car?"},
                        {"type": "image"}
                    ]
                }
            ]
            text = processor.apply_chat_template(messages, add_generation_prompt=True)

            # Process inputs
            inputs = processor(text=text, images=[image], return_tensors="pt", padding=True)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            # Generate predictions
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                pad_token_id=processor.tokenizer.pad_token_id
            )

            # Decode predictions
            pred_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]



            if "assistant" in pred_text:
                # Split after the last "assistant" occurrence
                assistant_response = pred_text.split("assistant")[-1].strip()

                # Remove any special tokens or prefixes
                assistant_response = re.sub(r'^[\s<]*', '', assistant_response)
                assistant_response = re.sub(r'<[^>]*>', '', assistant_response)
                assistant_response = assistant_response.strip()
            else:
                # If we can't find "assistant", use the whole text
                assistant_response = pred_text.strip()

            predictions.append(assistant_response)

    # Calculate accuracy - use case-insensitive comparison
    correct = 0
    comparison_results = []

    for p, a in zip(predictions, actuals):
        # Normalize strings for comparison
        p_norm = p.lower().strip()
        a_norm = a.lower().strip()

        # Check if predicted string contains actual string or vice versa
        match = (p_norm == a_norm) or (a_norm in p_norm) or (p_norm in a_norm)
        comparison_results.append(match)
        if match:
            correct += 1

    accuracy = correct / num_samples

    # Create visualization
    plt.figure(figsize=(20, 15))
    plt.suptitle(f"Model Evaluation - Accuracy: {accuracy:.1%} ({correct}/{num_samples})",
                 fontsize=16, y=0.98)

    for i in range(num_samples):
        plt.subplot(5, 2, i+1)
        plt.imshow(images[i])

        # Highlight correct predictions in green, incorrect in red
        color = "green" if comparison_results[i] else "red"
        plt.title(f"Actual: {actuals[i]}\nPredicted: {predictions[i]}",
                  fontsize=12, color=color)
        plt.axis("off")

    plt.tight_layout()
    plt.show()

    # Print detailed comparison for debugging
    print("\nDetailed Predictions:")
    print("-" * 80)
    for i in range(num_samples):
        match_status = "âœ“" if comparison_results[i] else "âœ—"
        print(f"Sample {i+1}: {match_status}")
        print(f"  Actual:    {actuals[i]}")
        print(f"  Predicted: {predictions[i]}")
        print("-" * 80)

    return accuracy

# Run evaluation on test set
accuracy = evaluate_model(
    model=model,
    processor=processor,
    dataset=ds["test"]
)

print(f"Final Accuracy: {accuracy:.2%}")